{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !mkdir data/harry_potter_txt\n",
        "!wget \"https://raw.githubusercontent.com/priyammaz/PyTorch-Adventures/main/data/harry_potter_txt/Book%201%20-%20The%20Philosopher's%20Stone.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nWtw5gYoiVf",
        "outputId": "7326a77a-549c-4fba-ab3f-3df7bdca1bdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-29 13:01:47--  https://raw.githubusercontent.com/priyammaz/PyTorch-Adventures/main/data/harry_potter_txt/Book%201%20-%20The%20Philosopher's%20Stone.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 492161 (481K) [text/plain]\n",
            "Saving to: ‘Book 1 - The Philosopher's Stone.txt’\n",
            "\n",
            "Book 1 - The Philos 100%[===================>] 480.63K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-04-29 13:01:48 (28.4 MB/s) - ‘Book 1 - The Philosopher's Stone.txt’ saved [492161/492161]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBLmJ137oMNh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset_dir_path= os.path.join(\"..\", \"data\", \"harry_potter_txt\")\n",
        "# files_indir= [ x for x in os.listdir(dataset_dir_path) if x.endswith('txt')]\n",
        "path1= \"/content/Book 1 - The Philosopher's Stone.txt\"\n",
        "\n",
        "allText=\"\"\n",
        "\n",
        "\n",
        "with open(path1, \"r\", encoding=\"utf-8\") as f:\n",
        "    text= f.readlines()\n",
        "    f.close()\n",
        "\n",
        "text = [line for line in text if \"Page\" not in line]\n",
        "text = \" \".join(text).replace(\"\\n\", \"\").lower()\n",
        "# text= word_tokenize(text)\n",
        "text = [word for word in text.split(\" \") if len(word) > 0]\n",
        "text = \" \".join(text)\n",
        "allText += text\n"
      ],
      "metadata": {
        "id": "SiqJSND5oe-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_chars = sorted(list(set(allText)))\n",
        "char2idx = {c:i for (i,c) in enumerate(unique_chars)}\n",
        "idx2char = {i:c for (i,c) in enumerate(unique_chars)}"
      ],
      "metadata": {
        "id": "ciYEQPvSpSYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataBuilder:\n",
        "    \"\"\"Randomely slice the data\"\"\"\n",
        "    def __init__(self, seq_len=100, text=allText):\n",
        "\n",
        "        self.seq_len = seq_len\n",
        "        self.text = text\n",
        "        self.file_length = len(text)\n",
        "\n",
        "    def grab_random_sample(self):\n",
        "\n",
        "        start = np.random.randint(0, self.file_length-self.seq_len)\n",
        "        end = start + self.seq_len\n",
        "        text_slice = self.text[start:end]\n",
        "        # print(start, end)\n",
        "        # print(text_slice)\n",
        "\n",
        "        input_text = text_slice[:-1]\n",
        "        label = text_slice[1:]\n",
        "\n",
        "        input_text = torch.tensor([char2idx[c] for c in input_text])\n",
        "        label = torch.tensor([char2idx[c] for c in label])\n",
        "\n",
        "        return input_text, label\n",
        "\n",
        "    def grab_random_batch(self, batch_size):\n",
        "\n",
        "        input_texts, labels = [], []\n",
        "\n",
        "        for _ in range(batch_size):\n",
        "            input_text, label = self.grab_random_sample()\n",
        "\n",
        "            input_texts.append(input_text)\n",
        "            labels.append(label)\n",
        "\n",
        "        input_texts = torch.stack(input_texts)\n",
        "        labels = torch.stack(labels)\n",
        "\n",
        "        return input_texts, labels"
      ],
      "metadata": {
        "id": "yOBa89S1pT2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMForGeneration(nn.Module):\n",
        "    def __init__(self, embedding_dim=128,\n",
        "                 num_characters=len(char2idx),\n",
        "                 hidden_size=256,\n",
        "                 n_layers=3,\n",
        "                 device=\"cpu\"):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_characters = num_characters #vocab\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.device = device\n",
        "\n",
        "        self.embedding = nn.Embedding(num_characters, embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
        "                            hidden_size=hidden_size,\n",
        "                            num_layers=n_layers,\n",
        "                            batch_first=True)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, num_characters)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1) # dim = -1 row wise (token wise ops)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x) # batchsize, seq_len, embedding dims\n",
        "        # print(x.shape)\n",
        "        op, (hn, cn) = self.lstm(x)\n",
        "        #op -> batch_size, seq_len, hidden_size\n",
        "        #hn -> n_layers, batch_size, hidden_size\n",
        "        #cn -> n_layers, batch_size, hidden_size\n",
        "        logits = self.fc(op) #batch_size, vocab_size\n",
        "        # logits= self.softmax(x) #batch_size, vocab_size\n",
        "        return logits\n",
        "\n",
        "    def write(self, text, max_characters, greedy=False):\n",
        "\n",
        "\n",
        "         idx = torch.tensor([char2idx[c] for c in text], device=self.device)\n",
        "         hidden = torch.zeros(self.n_layers, self.hidden_size).to(self.device)\n",
        "         cell = torch.zeros(self.n_layers, self.hidden_size).to(self.device)\n",
        "\n",
        "         for i in range(max_characters):\n",
        "             if i == 0:\n",
        "                selected_idx = idx\n",
        "             else:\n",
        "                selected_idx = idx[-1].unsqueeze(0)\n",
        "\n",
        "             x = self.embedding(selected_idx)\n",
        "             out, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
        "             out = self.fc(out)\n",
        "\n",
        "             if len(out) > 1:\n",
        "\n",
        "                out = out[-1, :].unsqueeze(0)\n",
        "\n",
        "\n",
        "             probs = self.softmax(out)\n",
        "\n",
        "             if greedy:\n",
        "                idx_next = torch.argmax(probs)\n",
        "             else:\n",
        "                idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "             idx = torch.cat([idx, idx_next[0]])\n",
        "\n",
        "         gen_string = [idx2char[int(c)] for c in idx]\n",
        "         gen_string = \"\".join(gen_string)\n",
        "\n",
        "         return gen_string\n",
        "\n",
        "\n",
        "model = LSTMForGeneration()\n",
        "text = \"hello\"\n",
        "model.write(text, 100, greedy=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "XZok0l1xpZgo",
        "outputId": "8fe2b048-4909-4cfc-9d9e-03f3df286cca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello(;/c(■”a4zs■bga‘y?•■gd05■!3vhsfx\"e)•?3ydwrpiy—xrdygnvh’d1\\\\0•:zakls•:l(x\"eut\\\\s,■j—? fay—mu■?i476fe0rh'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterations = 3000\n",
        "# iterations = 8\n",
        "max_len = 300\n",
        "evaluate_interval = 300\n",
        "embedding_dim = 128\n",
        "hidden_size = 256\n",
        "n_layers = 3\n",
        "lr = 0.003\n",
        "batch_size = 128\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = LSTMForGeneration(embedding_dim, len(char2idx), hidden_size, n_layers, DEVICE).to(DEVICE)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "dataset = DataBuilder()\n",
        "\n",
        "for iteration in range(iterations):\n",
        "    input_texts, labels = dataset.grab_random_batch(batch_size=batch_size)\n",
        "    input_texts, labels = input_texts.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output= model(input_texts)\n",
        "    output = output.transpose(1,2)\n",
        "\n",
        "    loss = loss_fn(output, labels) #tensor(4.1691, grad_fn=<NllLoss2DBackward0>)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if iteration % evaluate_interval == 0:\n",
        "        print(\"--------------------------------------\")\n",
        "        print(f\"Iteration {iteration}\")\n",
        "        print(f\"Loss {loss.item()}\")\n",
        "        generated_text = model.write(\"spells \", max_characters=200)\n",
        "        print(\"Sample Generation\")\n",
        "        print(generated_text)\n",
        "        print(\"--------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91Q-MGXVpcH5",
        "outputId": "a25101c6-b9b6-4263-fbc7-61435266445e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------\n",
            "Iteration 0\n",
            "Loss 4.036876201629639\n",
            "Sample Generation\n",
            "spells iw\"l“xb‘”16h”•a?t?,vph4u0apjl.•\")q1hvi8;j’a/—■e9\"y6\";x'’(9fc‘hx(/‘rr■q00b“lrk:■/fqaw7' z■b5(::hyjhf!\\/“\\/5als2;’)'!x\"8v;—-‘?,ad8‘6”“c39jw),orml1 6e'de6ti8e’vvcfk\"y,h‘r•8:,c2jxgga?'’?9\"68’ea f (!mw (n‘\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "Iteration 300\n",
            "Loss 1.5681543350219727\n",
            "Sample Generation\n",
            "spells and there tofter were be,” ...” mavate as wall, on the diend have day comclassing of couldn’t a calling witereon a tall seven to a before conce enestioned out, could normiled lesside had: harry was gl\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "Iteration 600\n",
            "Loss 1.2880799770355225\n",
            "Sample Generation\n",
            "spells the thind you-know i show. seak and fer again?” ben look. so retome his tobes weren’t na grange all mowk to cabby grand what sropped adring, the jatch that were here shout. it’ll keet sweak into the s\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "Iteration 900\n",
            "Loss 1.2384411096572876\n",
            "Sample Generation\n",
            "spells off, then to be safer on,” leaves a sut of shop pu akarding couranch, because a smell stumped and saying around strees. “i hupering.” “do — i’m aunt petunia dear mereing already’s looking at dotwerad,\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "Iteration 1200\n",
            "Loss 1.1329647302627563\n",
            "Sample Generation\n",
            "spells snake; they’re guardin’ his big brith — several moment, that,” said harry, he gatched thrapp onto a row, and mrs. dursley had did to complete disweess. then they had ten. no friend changing at the cor\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "Iteration 1500\n",
            "Loss 1.0744541883468628\n",
            "Sample Generation\n",
            "spells to us the doorstenited to take plant hermione stopped ron bartenders and crept streak and still didn’t are another refuse.” something think it wouldn’t have been letters one of the right to ask a pnoc\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "Iteration 1800\n",
            "Loss 1.0214688777923584\n",
            "Sample Generation\n",
            "spells toward me our business. .., not beings — that’s not easily at all. scatch anything. madam follow — “not’s flying,” harry saying harry pritedly into a stone up. something his armled she lies. harry tor\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "Iteration 2100\n",
            "Loss 0.9822626113891602\n",
            "Sample Generation\n",
            "spells off they were very interesting in his ears. he said. “thanks, too.” “and that’s sir, — and the first time touch me — i can ask me,” said harry suggested that scabbers finally. “flew and that’s going t\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "Iteration 2400\n",
            "Loss 0.9501376152038574\n",
            "Sample Generation\n",
            "spells from the platform nine and there is but the great the best and set off in your father is it — yer pause you.” harry was tricky old copnin? elevil casaged him as they hudged the sea and staircase. “it’\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "Iteration 2700\n",
            "Loss 0.8866034746170044\n",
            "Sample Generation\n",
            "spells of the thought anymore, had telling her sent this little vernon came out onto the from in the guarding jointing at his bowed, harry won’t better best bilick up to the rock to great library. hermione, \n",
            "--------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c0DHqhBdphb-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}